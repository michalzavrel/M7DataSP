{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zzDFiIUVwrHw",
        "6jGoxvZBbbOL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3ee2306c-8017-451e-8b3d-9dbbc00c6abd",
        "outputId": "8a347b26-b2e8-41b3-f29e-c82e200189b6",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
            "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m181.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "Successfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 huggingface-hub-0.26.2 multiprocess-0.70.16 pyarrow-18.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "e5ec69ec-5191-4a07-aed0-0e2226524a97",
        "outputId": "3c058aec-51a1-42e3-fd41-138d97c1fcb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df4ce989-ea7f-4c89-975c-046038996d13"
      },
      "outputs": [],
      "source": [
        "#import wandb  # we will talk about wandb next lecture\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6cd5135-2a36-4c86-a03a-3a890c5c2088"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"chromanna/hasek_dataset\")\n",
        "\n",
        "# Make validation split\n",
        "dataset = dataset['train'].train_test_split(test_size=0.0015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db3e836a-8548-496b-bdaf-d2b98f5e2596"
      },
      "outputs": [],
      "source": [
        "# load the gpt-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token=tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82b1dfeb-3191-414e-b6b9-0b198946d334",
        "outputId": "967ff50d-8326-4d97-978c-3b698f35ef14",
        "colab": {
          "referenced_widgets": [
            "893c1f2573ca4c98a9181835991f5deb",
            "ee0acc0b409041b691389d5455a8dc48"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "893c1f2573ca4c98a9181835991f5deb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee0acc0b409041b691389d5455a8dc48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'tokens', 'url', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 128\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'tokens', 'url', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenize the dataset\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(text=example[\"text\"], truncation=True, max_length=1024)\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98cdb38c-eb4f-47e7-8c43-96bacb7826ba",
        "outputId": "ce210b36-185b-4d95-95e1-a30a280b1fb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 251\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 2\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import chain\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def concatenate_and_chunk(dataset, chunk_size=512):\n",
        "    # Flatten all `input_ids` into a single list\n",
        "    all_input_ids = list(chain(*dataset[\"input_ids\"]))\n",
        "\n",
        "    # Create chunks of `chunk_size`\n",
        "    chunks = [all_input_ids[i:i + chunk_size] for i in range(0, len(all_input_ids), chunk_size)]\n",
        "\n",
        "    # Only keep chunks that are exactly of length `chunk_size`\n",
        "    chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
        "\n",
        "    # Create a new dataset with only the `input_ids` chunks\n",
        "    return Dataset.from_dict({\"input_ids\": chunks})\n",
        "\n",
        "# Apply this function to each split (train and test) in the DatasetDict\n",
        "chunked_ds = DatasetDict({\n",
        "    split: concatenate_and_chunk(split_ds, chunk_size=512)\n",
        "    for split, split_ds in tokenized_ds.items()\n",
        "})\n",
        "\n",
        "chunked_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36f1ef42-f488-4ca0-af36-464f3e535be8"
      },
      "outputs": [],
      "source": [
        "# data collator joins chunks into batches\n",
        "# see https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfe26fe6-925f-49ec-a854-ebdb429c66ad"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37301b62-4bea-4006-aa1a-7785924227b1"
      },
      "outputs": [],
      "source": [
        "# Define the model configuration for the smallest GPT-2\n",
        "config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),      # Standard GPT-2 vocab size 50257\n",
        "    n_positions=512,                # Context size (512 is enough for small-scale models)\n",
        "    n_embd=768,                     # Embedding size\n",
        "    n_layer=12,                     # Number of transformer layers\n",
        "    n_head=12,                      # Number of attention heads\n",
        ")\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7a48b42-9ce2-433f-acff-ce694449c317"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Define the perplexity metric\n",
        "def compute_metrics(eval_pred):\n",
        "    # `eval_pred` is a tuple of (logits, labels)\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n",
        "    if isinstance(logits, np.ndarray):\n",
        "        logits = torch.tensor(logits)\n",
        "    if isinstance(labels, np.ndarray):\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # Shift labels so that tokens align for calculating loss\n",
        "    shift_labels = labels[:, 1:].reshape(-1)\n",
        "    shift_logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
        "\n",
        "    # Calculate the cross-entropy loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens\n",
        "    loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = math.exp(loss.item())\n",
        "    return {\"perplexity\": perplexity}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ff4c203-9d5d-494e-b425-4b5a265516ff"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1e261bfb-52fe-4317-92cc-77f7c8b85b9f"
      },
      "outputs": [],
      "source": [
        "# pip install \"accelerate>=0.26.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ff634b-e879-44ad-a6d1-21c0a573aa0c",
        "outputId": "29bc6511-f71f-4f9c-bb3e-69b9ae9c85a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_77815/1040206857.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(model=model,\n"
          ]
        }
      ],
      "source": [
        "# Set this according to size of your dataset\n",
        "# You should train for at least 15 mins on A10 GPU to get something reasonable\n",
        "TRAIN_EPOCHS = 200\n",
        "\n",
        "SAVE_STEPS = 1000\n",
        "EVAL_STEPS = SAVE_STEPS // 2\n",
        "\n",
        "# training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-training\",  # Directory to save the model checkpoints and other outputs\n",
        "    eval_strategy=\"steps\",  # Evaluation strategy to use during training ('steps' or 'epochs')\n",
        "    eval_steps=EVAL_STEPS,  # Perform evaluation every EVAL_STEPS steps\n",
        "    num_train_epochs=TRAIN_EPOCHS,  # Total number of training epochs\n",
        "    per_device_train_batch_size=4,  # Batch size for training on each device\n",
        "    per_device_eval_batch_size=4,  # Batch size for evaluation on each device\n",
        "    learning_rate=1e-4,  # Initial learning rate for the optimizer\n",
        "    lr_scheduler_type='cosine',  # Learning rate scheduler type. 'cosine' provides a cosine decay schedule.\n",
        "    warmup_ratio=0.05,  # Proportion of training to perform linear learning rate warmup for\n",
        "    adam_beta1=0.9,  # Beta1 parameter for the Adam optimizer (first moment decay)\n",
        "    adam_beta2=0.999,  # Beta2 parameter for the Adam optimizer (second moment decay)\n",
        "    weight_decay=0.01,  # Weight decay to apply (L2 regularization)\n",
        "    logging_strategy=\"steps\",  # Logging strategy to use. 'steps' logs at specified steps.\n",
        "    logging_steps=EVAL_STEPS,  # Log training metrics every EVAL_STEPS steps\n",
        "    save_steps=SAVE_STEPS,  # Save a checkpoint every SAVE_STEPS steps\n",
        "    save_total_limit=10,  # Maximum number of checkpoints to keep. Older checkpoints are deleted.\n",
        "    # report_to='wandb',  # Uncomment to report metrics to Weights and Biases (optional)\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                 args = training_args,\n",
        "                 tokenizer=tokenizer,\n",
        "                 train_dataset=chunked_ds[\"train\"],\n",
        "                 eval_dataset=chunked_ds[\"test\"],\n",
        "                 compute_metrics=compute_metrics,\n",
        "                 data_collator = data_collator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f72a95fd-d7ac-4403-81d8-a42532f7a7a5",
        "outputId": "0b926eb5-0e76-4e48-d26a-cf7a19b6bac3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12600' max='12600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12600/12600 26:02, Epoch 200/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Perplexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.554700</td>\n",
              "      <td>3.911297</td>\n",
              "      <td>49.963979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.120800</td>\n",
              "      <td>4.331063</td>\n",
              "      <td>76.020584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.694500</td>\n",
              "      <td>5.332773</td>\n",
              "      <td>207.010968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.698200</td>\n",
              "      <td>5.978467</td>\n",
              "      <td>394.804872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.279100</td>\n",
              "      <td>6.470409</td>\n",
              "      <td>645.689844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.144200</td>\n",
              "      <td>6.459249</td>\n",
              "      <td>638.589231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>6.821347</td>\n",
              "      <td>917.146416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.071600</td>\n",
              "      <td>6.940217</td>\n",
              "      <td>1032.836759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.055300</td>\n",
              "      <td>7.224874</td>\n",
              "      <td>1373.083768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.043400</td>\n",
              "      <td>7.213590</td>\n",
              "      <td>1357.612412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>7.259552</td>\n",
              "      <td>1421.559182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.027700</td>\n",
              "      <td>7.342632</td>\n",
              "      <td>1544.620593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.022000</td>\n",
              "      <td>7.435630</td>\n",
              "      <td>1695.239487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>7.468189</td>\n",
              "      <td>1751.181026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>7.611944</td>\n",
              "      <td>2022.036145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>7.650607</td>\n",
              "      <td>2101.712835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.007700</td>\n",
              "      <td>7.875091</td>\n",
              "      <td>2630.262234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>7.785767</td>\n",
              "      <td>2405.855277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>7.913303</td>\n",
              "      <td>2733.425886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>8.006697</td>\n",
              "      <td>3000.910291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>8.063789</td>\n",
              "      <td>3177.143715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>8.099636</td>\n",
              "      <td>3292.989853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>8.136159</td>\n",
              "      <td>3415.251296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>8.160267</td>\n",
              "      <td>3498.779952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>8.162510</td>\n",
              "      <td>3506.613261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12600, training_loss=0.47303776493384725, metrics={'train_runtime': 1563.4809, 'train_samples_per_second': 32.108, 'train_steps_per_second': 8.059, 'total_flos': 1.31168600064e+16, 'train_loss': 0.47303776493384725, 'epoch': 200.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54b62744-896e-4faf-b21b-3e031dc1217d"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./gpt2-small-final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01639456-2124-46e8-8d6c-46b4cbd5c484",
        "outputId": "b46e534d-8f88-4e68-d5b5-2ecfedd178e3",
        "colab": {
          "referenced_widgets": [
            "2fad65aaaed64577b1df467ab292e734"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fad65aaaed64577b1df467ab292e734",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/michizavrel14/my_small_gpt2_hasek_dataset/commit/ca5f1c985864a17f81b034d3211536396db6b504', commit_message='Upload tokenizer', commit_description='', oid='ca5f1c985864a17f81b034d3211536396db6b504', pr_url=None, repo_url=RepoUrl('https://huggingface.co/michizavrel14/my_small_gpt2_hasek_dataset', endpoint='https://huggingface.co', repo_type='model', repo_id='michizavrel14/my_small_gpt2_hasek_dataset'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_MODEL_NAME = \"my_small_gpt2_hasek_dataset\"\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "model.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)\n",
        "tokenizer.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd6dac4-f46e-42a2-833f-07cb16eab3bc"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now you can switch from GPU to CPU. Try to complete some prompt specific to your dataset.\n",
        "\n",
        "Does it make sense? Is it at least in Czech/Slovak?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f9ade4a-f8b3-441d-8b6f-550048a21f37"
      },
      "outputs": [],
      "source": [
        "from transformers import  GPT2LMHeadModel, AutoTokenizer, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token=tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ee0298-2d7f-4aa4-9a2d-13429b7e197e",
        "outputId": "2a7a06cf-cafb-4869-ad31-f519e413a7e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "model =  GPT2LMHeadModel.from_pretrained(\"./gpt2-small-final\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e42697e6-10ad-4ea8-a9a9-addef655a1d6",
        "outputId": "fb23366c-20de-447e-a9d4-7281051ec0b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Zabili nám Ferdinanda,ostzy srdého. Lola přítě-licí bral: „Napr“ Překla panu a pole pos'}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PROMPT = \"Zabili nám Ferdinanda,\" # Set starting prompt, something specific for your dataset\n",
        "\n",
        "generator(\n",
        "    PROMPT,\n",
        "    max_length=50,       # Maximum length of the generated text\n",
        "    do_sample=True,\n",
        "    temperature=0.8,         # Experiment with this\n",
        "    repetition_penalty=1.8, # Experiment with this\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0b12a36-09d5-4e3b-99bd-1ea157012107"
      },
      "source": [
        "Now go back to your training folder `.gpt2-training/`. Each `checkpoint-N` folder contains the model saved after N steps.\n",
        "\n",
        "If you experiment with the older models, you should see that the models improves with time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e7a20e6-c47b-44e5-8d1a-2e6825dbef74"
      },
      "outputs": [],
      "source": [
        "def get_sample_after_N_steps(N, prompt, **kwargs):\n",
        "    model =  GPT2LMHeadModel.from_pretrained(f\"./gpt2-training/checkpoint-{N}/\")\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    output = generator(prompt, **kwargs)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fc850f1-9c94-4a71-b56b-a2eecc5eecf3",
        "outputId": "e1140c6e-426b-478c-ecf8-f4999860a4f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Pokus z dneou dky, kterátělou kdo'}]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_sample_after_N_steps(1000, \"Pokus\", do_sample=True, temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "035fbb7a-4147-435f-b9fc-f3c30a6eea1e",
        "outputId": "ab77940b-14c7-4a98-e5c9-ea8f9ef18b79",
        "collapsed": true
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Incorrect path_or_model_id: './gpt2-training/checkpoint-2000/'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './gpt2-training/checkpoint-2000/'. Use `repo_type` argument if needed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_sample_after_N_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPokus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m, in \u001b[0;36mget_sample_after_N_steps\u001b[0;34m(N, prompt, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sample_after_N_steps\u001b[39m(N, prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m  \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./gpt2-training/checkpoint-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mN\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m generator(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:3506\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3505\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3506\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3519\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3521\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3522\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
            "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './gpt2-training/checkpoint-2000/'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
          ]
        }
      ],
      "source": [
        "get_sample_after_N_steps(2000, \"Pokus\", do_sample=True, temperature=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b64f2fe5-9ada-497b-aff0-10b0bb327785"
      },
      "outputs": [],
      "source": [
        "get_sample_after_N_steps(3000, \"Pokus\", do_sample=True, temperature=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analýza kapitol Švejka\n",
        "\n",
        "Osudy dobrého vojáka Švejka za světové války (1921–1923) je čtyřdílný protiválečný humoristický román Jaroslava Haška. Jde o nejpřekládanější český román, v roce 2013 byl přeložený do 58 jazyků. Skládá se z následujících knih:\n",
        "\n",
        "1. V zázemí: prvních 17 kapitol (řádků)\n",
        "2. Na frontě: dalších 5 kapitol\n",
        "3. Slavný výprask: další 4 kapitoly\n",
        "4. Pokračování slavného výprasku: další 3 kapitoly\n",
        "\n",
        "Inspirací k sepsání románu Haškovi byly mj. jeho zážitky z působení v armádě za 1. světové války. Knihu nikdy nedopsal, protože zemřel na ochrnutí srdce. Ke konci psaní byl ve stavu, že nemohl psát, přesto dále diktoval kapitoly Švejka ve své ložnici.\n",
        "\n",
        "Je zajímavé zjistit **nakolik se jeho zdravotní stav a postup v práci (flow) propsaly do vyznění navazujících kapitol**.\n",
        "\n"
      ],
      "metadata": {
        "id": "zzDFiIUVwrHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Krok 1: Načtení datasetu \"hasek_dataset\"\n",
        "\n",
        "Za využití knihovny Hugging Face načteme dataset \"[hasek_dataset](https://huggingface.co/datasets/chromanna/hasek_dataset)\" v češtině.\n",
        "\n"
      ],
      "metadata": {
        "id": "i5Yk2SVha1fl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGvFptifZFbT"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets # uncomment in case of Colab\n",
        "from transformers import pipeline, MarianMTModel, MarianTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Načtení datasetu\n",
        "dataset = load_dataset(\"chromanna/hasek_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Krok 2: Funkce na přeložení textů do angličtiny\n",
        "\n",
        "S využitím modelu Helsinki-NLP/opus-mt-cs-en překládáme české texty do angličtiny."
      ],
      "metadata": {
        "id": "A_vsWQDpbN7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# knihovna sacremoses je doporučena pro správnou funkci tokenizace s modelem MarianMT.\n",
        "# sacremoses se používá k pokročilému zpracování textu, které může zlepšit kvalitu překladu.\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "QxEx3xoFlVI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Načtení překladu modelu a tokenizéru pro češtinu -> angličtinu\n",
        "model_name = \"Helsinki-NLP/opus-mt-cs-en\"\n",
        "translation_model = MarianMTModel.from_pretrained(model_name)\n",
        "translation_tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Překlad textu do angličtiny\n",
        "def translate_to_english(text):\n",
        "    inputs = translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    translated = translation_model.generate(**inputs)\n",
        "    return translation_tokenizer.decode(translated[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
      ],
      "metadata": {
        "id": "Rxbg68LYbK3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Krok 3: Detekce nenávistných projevů\n",
        "\n",
        "Pomocí modelu \"multilingual-hate-speech-robacofi\" aplikujeme klasifikaci nenávistného obsahu na přeložené texty. Pro každý vstup zobrazíme původní text, překlad a predikci. Kód níže pracuje s prvními 29 záznamy v datasetu, které zahrnují kompletní román. Pro větší dataset nebo zpracování veškerého Haškova díla může být potřeba přidat dávkování (batching) pro efektivnější běh a lepší správu paměti.\n"
      ],
      "metadata": {
        "id": "6jGoxvZBbbOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Načtení modelu pro detekci nenávistných projevů\n",
        "hate_speech_classifier = pipeline(\"text-classification\", model=\"Andrazp/multilingual-hate-speech-robacofi\")\n",
        "max_length = 512  # Maximální délka v tokenech\n",
        "\n",
        "# Použití překladu a detekce nenávistných projevů na vzorek dat\n",
        "n = 29 # kolik textů chci analyzovat (Komplet Švejk)\n",
        "translated_texts = [translate_to_english(text) for text in dataset[\"train\"][:n]['text']]  # Prvních n textů pro ukázku"
      ],
      "metadata": {
        "id": "OBR0gW1Jau_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializace seznamu pro uložení všech predikcí\n",
        "all_predictions = []\n",
        "\n",
        "# Process each translated text individually\n",
        "for translated_text in translated_texts:\n",
        "    tokens = hate_speech_classifier.tokenizer(translated_text, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    truncated_text = hate_speech_classifier.tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    prediction = hate_speech_classifier(truncated_text)\n",
        "    all_predictions.append(prediction)"
      ],
      "metadata": {
        "id": "HCPd4Ma10fiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zobrazení výsledků - kontrola\n",
        "for title, original, translated, prediction in zip(dataset[\"train\"][:n][\"title\"], dataset[\"train\"][:n][\"text\"], translated_texts, all_predictions):\n",
        "    print(f\"Název: {title}\")\n",
        "    print(f\"Původní text: {original[:50]}\")\n",
        "    print(f\"Přeloženo: {translated[:50]}\")\n",
        "    print(f\"Predikce: {prediction}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c3wfcbeaZT",
        "outputId": "08437991-5be4-46ea-92e7-58f6b9b6ad0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Název: Úvod\n",
            "Původní text: Veliká doba žádá velké lidi. Jsou nepoznaní hrdino\n",
            "Přeloženo: Today you can meet in the streets of Prague a scra\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.8561887145042419}]\n",
            "==================================================\n",
            "Název: Zasáhnutí dobrého vojáka Švejka do světové války\n",
            "Původní text: „Tak nám zabili Ferdinanda,“ řekla posluhovačka pa\n",
            "Přeloženo: So Ferdinand was killed, and so did Mrs.Mr.Mr.Mr.M\n",
            "Predikce: [{'label': 'offensive', 'score': 0.9532903432846069}]\n",
            "==================================================\n",
            "Název: Dobrý voják Švejk na policejním ředitelství\n",
            "Původní text: Sarajevský atentát naplnil policejní ředitelství č\n",
            "Přeloženo: This was the case with two people, and the old ins\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9995518326759338}]\n",
            "==================================================\n",
            "Název: Švejk před soudními lékaři\n",
            "Původní text: Čisté, útulné pokojíky zemského „co trestního“ sou\n",
            "Přeloženo: Clean, cozy rooms of the earth were the same as th\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9840944409370422}]\n",
            "==================================================\n",
            "Název: Švejka vyhodili z blázince\n",
            "Původní text: Když později Švejk líčil život v blázinci, činil t\n",
            "Přeloženo: When the old man spent his life in a madhouse, he \n",
            "Predikce: [{'label': 'not offensive', 'score': 0.8068269491195679}]\n",
            "==================================================\n",
            "Název: Švejk na policejním ředitelství v Salmově ulici\n",
            "Původní text: Po krásných slunných dnech v blázinci přišly na Šv\n",
            "Přeloženo: After a beautiful sunny day, you walk away from th\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9832130074501038}]\n",
            "==================================================\n",
            "Název: Švejk opět doma, proraziv začarovaný kruh\n",
            "Původní text: Budovou policejního ředitelství vanul duch cizí au\n",
            "Přeloženo: In addition to a few exceptions, people who didn't\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.7238047122955322}]\n",
            "==================================================\n",
            "Název: Švejk jde na vojnu\n",
            "Původní text: V době, kdy lesy na řece Rábu v Haliči viděly utík\n",
            "Přeloženo: At the time when the forests on the river Rábéhoho\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.998859167098999}]\n",
            "==================================================\n",
            "Název: Švejk simulantem\n",
            "Původní text: V této velké době vojenští lékaři dali si neobyčej\n",
            "Přeloženo: In this great time, the military doctors put the h\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9990960359573364}]\n",
            "==================================================\n",
            "Název: Švejk na garnizóně\n",
            "Původní text: Posledním útočištěm lidí, kteří nechtěli jít do vá\n",
            "Přeloženo: The last refuge of people who didn't want to go in\n",
            "Predikce: [{'label': 'offensive', 'score': 0.9884794354438782}]\n",
            "==================================================\n",
            "Název: Švejk vojenským sluhou u polního kuráta\n",
            "Původní text: Znovu počíná jeho odysea pod čestným průvodem dvou\n",
            "Přeloženo: Once again, his odyssey started under the honorary\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9986175298690796}]\n",
            "==================================================\n",
            "Název: Švejk jede s polním kurátem sloužit polní mši\n",
            "Původní text: Přípravy k usmrcování lidí děly se vždy jménem bož\n",
            "Přeloženo: Before the ancient Phoenicians cut their throats o\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9556806087493896}]\n",
            "==================================================\n",
            "Název: Náboženská debata\n",
            "Původní text: Stávalo se, že Švejk po celé dny neviděl pěstitele\n",
            "Přeloženo: It happened that Švejk has not seen the stock and \n",
            "Predikce: [{'label': 'not offensive', 'score': 0.998417854309082}]\n",
            "==================================================\n",
            "Název: Švejk jde zaopatřovat\n",
            "Původní text: Polní kurát Otto Katz seděl zadumaně nad cirkuláře\n",
            "Přeloženo: The field court Otto Katz sat for the half of the \n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9972084164619446}]\n",
            "==================================================\n",
            "Název: Švejk vojenským sluhou u nadporučíka Lukáše\n",
            "Původní text: Štěstí Švejkovo nemělo dlouhého trvání. Nelítostný\n",
            "Přeloženo: If the field court up to that event was a person w\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9986456036567688}]\n",
            "==================================================\n",
            "Název: Katastrofa\n",
            "Původní text: Plukovník Bedřich Kraus, mající též přídomek von Z\n",
            "Přeloženo: The colonel Bedřich Kraus, who also had a large pa\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9220117926597595}]\n",
            "==================================================\n",
            "Název: Doslov k prvnímu dílu „V zázemí“\n",
            "Původní text: Ukončuje první díl knihy Osudy dobrého vojáka Švej\n",
            "Přeloženo: The first part of the book The fates of the good s\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9961327314376831}]\n",
            "==================================================\n",
            "Název: Švejkovy nehody ve vlaku\n",
            "Původní text: V jednom kupé druhé třídy rychlíku Praha-České Bud\n",
            "Přeloženo: In one of the second classes of the speed bank Pra\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9994022846221924}]\n",
            "==================================================\n",
            "Název: Švejkova budějovická anabaze\n",
            "Původní text: Starověký válečník Xenofón prošel celou Malou Asii\n",
            "Přeloženo: The old Goths made their expeditions also without \n",
            "Predikce: [{'label': 'offensive', 'score': 0.7062597274780273}]\n",
            "==================================================\n",
            "Název: Švejkovy příhody v Királyhidě\n",
            "Původní text: Jednadevadesátý pluk se stěhoval do Mostu nad Lita\n",
            "Přeloženo: After three days of imprisonment, Shvejk was relea\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9964075684547424}]\n",
            "==================================================\n",
            "Název: Nová utrpení\n",
            "Původní text: Plukovník Schröder se zalíbením pozoroval bledý ob\n",
            "Přeloženo: Colonel Schröder is pleased with the whole case of\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9845089316368103}]\n",
            "==================================================\n",
            "Název: Z Mostu nad Litavou k Sokalu\n",
            "Původní text: Nadporučík Lukáš chodil rozčileně po kanceláři 11.\n",
            "Přeloženo: Lukáš Lukáš was angry with the office of the 11th \n",
            "Predikce: [{'label': 'not offensive', 'score': 0.8708106875419617}]\n",
            "==================================================\n",
            "Název: Přes Uhry\n",
            "Původní text: Konečně se všichni dočkali toho okamžiku, kdy je n\n",
            "Přeloženo: Finally, all of them lived up to that moment, and \n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9996090531349182}]\n",
            "==================================================\n",
            "Název: V Budapešti\n",
            "Původní text: Matušič přinesl na vojenském nádraží v Budapešti h\n",
            "Přeloženo: Matušič brought it to the military station in Buda\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9998263716697693}]\n",
            "==================================================\n",
            "Název: Z Hatvanu na hranice Haliče\n",
            "Původní text: Po celou dobu železniční přepravy bataliónu, který\n",
            "Přeloženo: During the entire railway transport of the battali\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9710646271705627}]\n",
            "==================================================\n",
            "Název: Marschieren marsch!\n",
            "Původní text: Objevilo se, když se přijelo do Sanoku, že vlastně\n",
            "Přeloženo: When you came to Sanoku, you told me that there wo\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.999584972858429}]\n",
            "==================================================\n",
            "Název: Švejk v transportu ruských zajatců\n",
            "Původní text: Když tedy Švejk, považovaný omylem v ruském plášti\n",
            "Přeloženo: When Švejk, considered by mistake in the Russian c\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.9965963959693909}]\n",
            "==================================================\n",
            "Název: Duchovní útěcha\n",
            "Původní text: Polní kurát Martinec v pravém slova smyslu nepřiše\n",
            "Přeloženo: I mean, I didn't come to this place, I didn't come\n",
            "Predikce: [{'label': 'not offensive', 'score': 0.998008668422699}]\n",
            "==================================================\n",
            "Název: Švejk opět u své marškumpanie\n",
            "Původní text: Onen major, který dělal auditora při včerejším dop\n",
            "Přeloženo: The major, who was the auditor of the other day, a\n",
            "Predikce: [{'label': 'offensive', 'score': 0.9168186187744141}]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analýza jednotlivých kapitol Švejka."
      ],
      "metadata": {
        "id": "i7uailNo4TLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Vytvoření DataFrame s výsledky\n",
        "df_hate = pd.DataFrame({\n",
        "    \"Title\": dataset[\"train\"][\"title\"][:n],\n",
        "    \"Label\": [pred[0]['label'] for pred in all_predictions],\n",
        "    \"Score\": [pred[0]['score'] for pred in all_predictions]\n",
        "})\n",
        "\n",
        "# Použití funkce k extrahování labelů a skóre výběr řádků pro každou kapitolu)\n",
        "chapter_1_lines = df_hate[:17]  # Prvních 17 řádků\n",
        "chapter_2_lines = df_hate[17:22]  # Dalších 5 řádků\n",
        "chapter_3_lines = df_hate[22:26]  # Další 4 řádky\n",
        "chapter_4_lines = df_hate[26:29]  # Další 3 řádky"
      ],
      "metadata": {
        "id": "jVVFTzvNwp8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_4_lines # kontola"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "iqmVc2Y69r3-",
        "outputId": "1be7640e-bcc4-4f84-a7fb-7992fe9de1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title          Label     Score\n",
              "26  Švejk v transportu ruských zajatců  not offensive  0.996596\n",
              "27                     Duchovní útěcha  not offensive  0.998009\n",
              "28       Švejk opět u své marškumpanie      offensive  0.916819"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2e4b10d-25ad-4446-94b8-0ac00406f138\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Label</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Švejk v transportu ruských zajatců</td>\n",
              "      <td>not offensive</td>\n",
              "      <td>0.996596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Duchovní útěcha</td>\n",
              "      <td>not offensive</td>\n",
              "      <td>0.998009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Švejk opět u své marškumpanie</td>\n",
              "      <td>offensive</td>\n",
              "      <td>0.916819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2e4b10d-25ad-4446-94b8-0ac00406f138')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2e4b10d-25ad-4446-94b8-0ac00406f138 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2e4b10d-25ad-4446-94b8-0ac00406f138');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a54c3db0-56b4-4fc5-82e1-509f60282d01\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a54c3db0-56b4-4fc5-82e1-509f60282d01')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a54c3db0-56b4-4fc5-82e1-509f60282d01 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_184e41d6-b2a2-4716-b1bf-6c113b71a52d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('chapter_4_lines')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_184e41d6-b2a2-4716-b1bf-6c113b71a52d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('chapter_4_lines');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "chapter_4_lines",
              "summary": "{\n  \"name\": \"chapter_4_lines\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"\\u0160vejk v transportu rusk\\u00fdch zajatc\\u016f\",\n          \"Duchovn\\u00ed \\u00fat\\u011bcha\",\n          \"\\u0160vejk op\\u011bt u sv\\u00e9 mar\\u0161kumpanie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"offensive\",\n          \"not offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04647277412554707,\n        \"min\": 0.9168186187744141,\n        \"max\": 0.998008668422699,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9965963959693909,\n          0.998008668422699\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice funkce pro výpočet požadovaných statistik\n",
        "def analyze_chapter(df):\n",
        "    # Počet řádků\n",
        "    row_count = len(df)\n",
        "\n",
        "    # Průměrné skóre\n",
        "    avg_score = round(df['Score'].mean(), 5)\n",
        "\n",
        "    # Nejčastější hodnota v 'Label' (medián labelu)\n",
        "    median_label = df['Label'].mode()[0]\n",
        "\n",
        "    # Počet nejčastějších hodnot\n",
        "    median_count = df['Label'].value_counts()[median_label]\n",
        "\n",
        "    # Procentuální zastoupení této hodnoty\n",
        "    median_label_percentage = round(df['Label'].value_counts(normalize=True)[median_label] * 100, 3)\n",
        "\n",
        "    return row_count, avg_score, median_label, median_count, median_label_percentage"
      ],
      "metadata": {
        "id": "2OjRANb0oHRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analýza pro každou kapitolu\n",
        "chapter_1_stats = analyze_chapter(chapter_1_lines)\n",
        "chapter_2_stats = analyze_chapter(chapter_2_lines)\n",
        "chapter_3_stats = analyze_chapter(chapter_3_lines)\n",
        "chapter_4_stats = analyze_chapter(chapter_4_lines)\n",
        "\n",
        "# Výsledky\n",
        "print(f\"Kapitola 1 - Průměrné skóre: {chapter_1_stats[1]}, Nejčastější label: {chapter_1_stats[2]}, Procento: {chapter_1_stats[4]}% ({chapter_1_stats[3]} ze {chapter_1_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 2 - Průměrné skóre: {chapter_2_stats[1]}, Nejčastější label: {chapter_2_stats[2]}, Procento: {chapter_2_stats[4]}% ({chapter_2_stats[3]} z {chapter_2_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 3 - Průměrné skóre: {chapter_3_stats[1]}, Nejčastější label: {chapter_3_stats[2]}, Procento: {chapter_3_stats[4]}% ({chapter_3_stats[3]} z {chapter_3_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 4 - Průměrné skóre: {chapter_4_stats[1]}, Nejčastější label: {chapter_4_stats[2]}, Procento: {chapter_4_stats[4]}% ({chapter_4_stats[3]} z {chapter_4_stats[0]} kapitol)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP9s_qt99yk8",
        "outputId": "72a63755-74e2-421c-b804-3f7172a5dab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kapitola 1 - Průměrné skóre: 0.9506, Nejčastější label: not offensive, Procento: 88.235% (15 ze 17 kapitol)\n",
            "Kapitola 2 - Průměrné skóre: 0.91148, Nejčastější label: not offensive, Procento: 80.0% (4 z 5 kapitol)\n",
            "Kapitola 3 - Průměrné skóre: 0.99252, Nejčastější label: not offensive, Procento: 100.0% (4 z 4 kapitol)\n",
            "Kapitola 4 - Průměrné skóre: 0.97047, Nejčastější label: not offensive, Procento: 66.667% (2 z 3 kapitol)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "S horšícím zdravotním stavem autora se skóre nenávistného textu rapidně nezhoršilo. Přičemž přiřazení správného tónu textu je celkem jisté (viz vysoké skóre). Ale je diskutabilní, zda analýza na malém počtu podkapitol je relevantní. Navíc do modelu vstupuje omezený počet tokenů (max 512). V našem případě jsme tam dali prvních 512 tokenů každé podkapitoly, ale třeba jsou naopak více plné frustrace závěry podkapitol :-)"
      ],
      "metadata": {
        "id": "unBev5zk-n-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Krok 4: Analýzu sentimentu"
      ],
      "metadata": {
        "id": "lxmBjfq6_D2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pro analýzu sentimentu na datasetu chromanna/hasek_dataset můžeme využít model [cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual), který je navržen pro vícejazyčnou analýzu sentimentu."
      ],
      "metadata": {
        "id": "pMwehB1Nq7Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Načtení tokenizeru a modelu pro analýzu sentimentu\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Vytvoření pipeline pro analýzu sentimentu\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, truncation=True, max_length=512)\n",
        "\n",
        "# Výběr prvních 29 řádků a jejich textů pro analýzu\n",
        "texts = dataset[\"train\"][\"text\"][:29]\n",
        "\n",
        "# Provádění analýzy sentimentu\n",
        "sentiments = sentiment_analyzer(texts)"
      ],
      "metadata": {
        "id": "mMjWscBg_JOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vyhodnocení pro jednotlivé hlavní kapitoly."
      ],
      "metadata": {
        "id": "hAzQcpFFrHOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vytvoření DataFrame s výsledky\n",
        "df_sentiment = pd.DataFrame({\n",
        "    \"Title\": dataset[\"train\"][\"title\"][:29],\n",
        "    \"Label\": [result['label'] for result in sentiments],\n",
        "    \"Score\": [result['score'] for result in sentiments]\n",
        "})\n",
        "\n",
        "# Rozdělení na hlavní kapitoly\n",
        "chapter_1_lines = df_sentiment[:17] # Prvních 17 řádků\n",
        "chapter_2_lines = df_sentiment[17:22]  # Dalších 5 řádků\n",
        "chapter_3_lines = df_sentiment[22:26]  # Další 4 řádky\n",
        "chapter_4_lines = df_sentiment[26:29]  # Další 3 řádky"
      ],
      "metadata": {
        "id": "1LuvVDwgjfwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analýza pro každou kapitolu\n",
        "chapter_1_stats = analyze_chapter(chapter_1_lines)\n",
        "chapter_2_stats = analyze_chapter(chapter_2_lines)\n",
        "chapter_3_stats = analyze_chapter(chapter_3_lines)\n",
        "chapter_4_stats = analyze_chapter(chapter_4_lines)\n",
        "\n",
        "# Výsledky\n",
        "print(f\"Kapitola 1 - Průměrné skóre: {chapter_1_stats[1]}, Nejčastější label: {chapter_1_stats[2]}, Procento: {chapter_1_stats[4]}% ({chapter_1_stats[3]} ze {chapter_1_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 2 - Průměrné skóre: {chapter_2_stats[1]}, Nejčastější label: {chapter_2_stats[2]}, Procento: {chapter_2_stats[4]}% ({chapter_2_stats[3]} z {chapter_2_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 3 - Průměrné skóre: {chapter_3_stats[1]}, Nejčastější label: {chapter_3_stats[2]}, Procento: {chapter_3_stats[4]}% ({chapter_3_stats[3]} z {chapter_3_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 4 - Průměrné skóre: {chapter_4_stats[1]}, Nejčastější label: {chapter_4_stats[2]}, Procento: {chapter_4_stats[4]}% ({chapter_4_stats[3]} z {chapter_4_stats[0]} kapitol)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1w2dcJ4luM8",
        "outputId": "fa1fdfce-9d37-48ed-867a-57795e3dcc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kapitola 1 - Průměrné skóre: 0.54387, Nejčastější label: neutral, Procento: 94.118% (16 ze 17 kapitol)\n",
            "Kapitola 2 - Průměrné skóre: 0.53562, Nejčastější label: neutral, Procento: 100.0% (5 z 5 kapitol)\n",
            "Kapitola 3 - Průměrné skóre: 0.52247, Nejčastější label: neutral, Procento: 100.0% (4 z 4 kapitol)\n",
            "Kapitola 4 - Průměrné skóre: 0.46977, Nejčastější label: neutral, Procento: 100.0% (3 z 3 kapitol)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Většina podkapitol byla vyhodnocena jako neutrální. Není patrné, že by se nálada a vyznění podkapitol ke konci autorova života, kdy už neměl sílu ani psát, nějak zhoršovala. Model vykazuje menší jistotu správně přiřazeného sentimentu (skóre $≈$ .5). Opět je diskutabilní malý počet podkapitol a jak se text ořízl (v našem případě prvních 512 tokenů každé podkapitoly).\n"
      ],
      "metadata": {
        "id": "klCyFGlHmB1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Krok 5: Detekce emocí"
      ],
      "metadata": {
        "id": "QqItBJqG_Juo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pro analýzu emocí v českém textu můžete využít model [bert-base-multilingual-cased-finetuned-emotion](https://huggingface.co/Toshifumi/bert-base-multilingual-cased-finetuned-emotion), který je dostupný na platformě Hugging Face. Tento model byl natrénován na rozpoznávání emocí v různých jazycích a je schopen zpracovávat i české texty."
      ],
      "metadata": {
        "id": "XdH6UQwsrY5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Načtení tokenizeru a modelu pro analýzu emocí\n",
        "model_name = \"Toshifumi/bert-base-multilingual-cased-finetuned-emotion\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Vytvoření pipeline pro analýzu emocí\n",
        "emotion_analyzer = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Provádění analýzy emocí\n",
        "emotions = emotion_analyzer(texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "Y8Dj8zul_DNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model je doladěn pro detekci následujících šesti emocí:"
      ],
      "metadata": {
        "id": "05Sltg_xv6Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice slovníku pro přemapování labelů\n",
        "label_map = {\n",
        "    \"LABEL_0\": \"sadness\",\n",
        "    \"LABEL_1\": \"joy\",\n",
        "    \"LABEL_2\": \"love\",\n",
        "    \"LABEL_3\": \"anger\",\n",
        "    \"LABEL_4\": \"fear\",\n",
        "    \"LABEL_5\": \"surprise\"\n",
        "}"
      ],
      "metadata": {
        "id": "1B3srUzRrkrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toto přiřazení štítků bylo aktualizováno [v souboru config.json](https://huggingface.co/Toshifumi/bert-base-multilingual-cased-finetuned-emotion/discussions/1/files) modelu, aby odpovídalo datové sadě použití pro trénink.\n",
        "\n",
        "Vyhodnocení pro jednotlivé hlavní kapitoly."
      ],
      "metadata": {
        "id": "eE3tk5qry2zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vytvoření DataFrame s výsledky\n",
        "df_emotion = pd.DataFrame({\n",
        "    \"Text\": texts,\n",
        "    \"Label\": [label_map[emotion['label']] for emotion in emotions],  # Přemapování labelů\n",
        "    \"Score\": [emotion['score'] for emotion in emotions]\n",
        "})\n",
        "\n",
        "# Rozdělení na hlavní kapitoly\n",
        "chapter_1_lines = df_emotion[:17] # Prvních 17 řádků\n",
        "chapter_2_lines = df_emotion[17:22]  # Dalších 5 řádků\n",
        "chapter_3_lines = df_emotion[22:26]  # Další 4 řádky\n",
        "chapter_4_lines = df_emotion[26:29]  # Další 3 řádky"
      ],
      "metadata": {
        "id": "Vx-pVk4jyZ0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analýza pro každou kapitolu\n",
        "chapter_1_stats = analyze_chapter(chapter_1_lines)\n",
        "chapter_2_stats = analyze_chapter(chapter_2_lines)\n",
        "chapter_3_stats = analyze_chapter(chapter_3_lines)\n",
        "chapter_4_stats = analyze_chapter(chapter_4_lines)\n",
        "\n",
        "# Výsledky\n",
        "print(f\"Kapitola 1 - Průměrné skóre: {chapter_1_stats[1]}, Nejčastější label: {chapter_1_stats[2]}, Procento: {chapter_1_stats[4]}% ({chapter_1_stats[3]} ze {chapter_1_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 2 - Průměrné skóre: {chapter_2_stats[1]}, Nejčastější label: {chapter_2_stats[2]}, Procento: {chapter_2_stats[4]}% ({chapter_2_stats[3]} z {chapter_2_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 3 - Průměrné skóre: {chapter_3_stats[1]}, Nejčastější label: {chapter_3_stats[2]}, Procento: {chapter_3_stats[4]}% ({chapter_3_stats[3]} z {chapter_3_stats[0]} kapitol)\")\n",
        "print(f\"Kapitola 4 - Průměrné skóre: {chapter_4_stats[1]}, Nejčastější label: {chapter_4_stats[2]}, Procento: {chapter_4_stats[4]}% ({chapter_4_stats[3]} z {chapter_4_stats[0]} kapitol)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObSViZxWwBF6",
        "outputId": "b75ba438-a93a-4765-e6b1-7a0bb9981e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kapitola 1 - Průměrné skóre: 0.35453, Nejčastější label: joy, Procento: 100.0% (17 ze 17 kapitol)\n",
            "Kapitola 2 - Průměrné skóre: 0.42902, Nejčastější label: joy, Procento: 100.0% (5 z 5 kapitol)\n",
            "Kapitola 3 - Průměrné skóre: 0.49682, Nejčastější label: joy, Procento: 100.0% (4 z 4 kapitol)\n",
            "Kapitola 4 - Průměrné skóre: 0.30083, Nejčastější label: joy, Procento: 100.0% (3 z 3 kapitol)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Všechny podkapitoly byly vyhodnoceny jako radostné. Není patrné, že by se emoce v textu ke konci autorova života nějak vyostřovaly. Model vykazuje ještě menší jistotu správně přiřazené emoce (skóre $≈$ .4) než jiné výše zkoušené modely analýzy textu."
      ],
      "metadata": {
        "id": "O0A6Rfj1zFFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Závěr analýzy textu\n",
        "\n",
        "Na základě různých analýz textu nevyplynulo, že by se rapidně zhoršující se fyzický stav Jaroslava Haška výrazně promítl do podoby kapitol románu o Švejkovi. A to i přes skutečnost, že knihu autor nestihl dopsat a ke konci ji už pouze diktoval. Text si po celou dobu s vysokou mírou jistoty drží nekonfliktní postoj, s nižší jistotou neutrální tón a radostnou náladu."
      ],
      "metadata": {
        "id": "1iQ1YHiW2hd2"
      }
    }
  ]
}